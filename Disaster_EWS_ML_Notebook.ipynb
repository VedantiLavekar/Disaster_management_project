{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1680c6a",
   "metadata": {},
   "source": [
    "# Disaster Prediction & Early Warning System (ML + GIS) — Without IoT\n",
    "\n",
    "**End-to-end Jupyter Notebook** that you can run directly. It focuses on:\n",
    "- Multi-hazard risk modeling (Flood & Drought)\n",
    "- Robust preprocessing & feature engineering\n",
    "- High-accuracy models with cross-validation + hyperparameter search\n",
    "- Explainability (Permutation Importance)\n",
    "- GIS risk mapping with Folium\n",
    "- Early warning rule engine + notifications (simulated)\n",
    "\n",
    "**Tip:** Replace the synthetic data section with your real CSV files when ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fec99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b65c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbae93a",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "Install/Import libraries. If a library is missing, run the pip cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d05485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\vedan\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\vedan\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vedan\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vedan\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Collecting folium\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\vedan\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Collecting shapely\n",
      "  Downloading shapely-2.1.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyproj\n",
      "  Downloading pyproj-3.7.2-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: requests in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from folium) (2022.9.0)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.11.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from pyproj) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from requests->folium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from requests->folium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vedan\\anaconda3\\lib\\site-packages (from requests->folium) (2.5.0)\n",
      "Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "Downloading shapely-2.1.1-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "Downloading pyproj-3.7.2-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.3 MB 87.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Downloading pyogrio-0.11.1-cp312-cp312-win_amd64.whl (19.2 MB)\n",
      "   ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/19.2 MB 24.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.4/19.2 MB 6.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.9/19.2 MB 5.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.4/19.2 MB 4.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.9/19.2 MB 3.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.9/19.2 MB 3.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 4.2/19.2 MB 3.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 4.2/19.2 MB 3.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.5/19.2 MB 2.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 4.7/19.2 MB 2.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 5.0/19.2 MB 2.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 5.0/19.2 MB 2.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 5.2/19.2 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.5/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 5.8/19.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 6.0/19.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.3/19.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.6/19.2 MB 1.7 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 6.8/19.2 MB 1.7 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 7.1/19.2 MB 1.7 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 7.3/19.2 MB 1.7 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 7.6/19.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.1/19.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 8.4/19.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 8.7/19.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 9.2/19.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 9.4/19.2 MB 1.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 9.7/19.2 MB 1.7 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 10.2/19.2 MB 1.7 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 10.5/19.2 MB 1.7 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 11.0/19.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 11.5/19.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 11.8/19.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 12.3/19.2 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.6/19.2 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.8/19.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.4/19.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.6/19.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 14.2/19.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 14.4/19.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.9/19.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 15.5/19.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 15.7/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.3/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.8/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.0/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.6/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.1/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.1/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.2/19.2 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely, pyproj, pyogrio, branca, geopandas, folium\n",
      "Successfully installed branca-0.8.1 folium-0.20.0 geopandas-1.1.1 pyogrio-0.11.1 pyproj-3.7.2 shapely-2.1.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy pandas scikit-learn matplotlib folium joblib shapely geopandas pyproj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e6b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffbf4b",
   "metadata": {},
   "source": [
    "## 2) Data\n",
    "We use **synthetic-yet-realistic** geospatial timeseries data to ensure the notebook runs anywhere. You can later switch to real datasets by replacing the synthetic data cells with `pd.read_csv(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a16d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset shape: (2500, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>rain_mm</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>wind_kph</th>\n",
       "      <th>humidity</th>\n",
       "      <th>river_level_m</th>\n",
       "      <th>soil_moisture</th>\n",
       "      <th>evap_mm</th>\n",
       "      <th>flood_event</th>\n",
       "      <th>drought_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.708241</td>\n",
       "      <td>74.350763</td>\n",
       "      <td>27.172204</td>\n",
       "      <td>26.853746</td>\n",
       "      <td>17.415451</td>\n",
       "      <td>66.403607</td>\n",
       "      <td>1.496472</td>\n",
       "      <td>30.674063</td>\n",
       "      <td>6.343199</td>\n",
       "      <td>0</td>\n",
       "      <td>67.650480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>19.033959</td>\n",
       "      <td>74.442670</td>\n",
       "      <td>69.529848</td>\n",
       "      <td>29.004376</td>\n",
       "      <td>13.874468</td>\n",
       "      <td>76.906338</td>\n",
       "      <td>3.319376</td>\n",
       "      <td>50.550377</td>\n",
       "      <td>8.022703</td>\n",
       "      <td>1</td>\n",
       "      <td>49.862756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.550919</td>\n",
       "      <td>73.632379</td>\n",
       "      <td>34.844697</td>\n",
       "      <td>27.977871</td>\n",
       "      <td>17.720272</td>\n",
       "      <td>62.174314</td>\n",
       "      <td>1.714159</td>\n",
       "      <td>32.428313</td>\n",
       "      <td>8.671048</td>\n",
       "      <td>0</td>\n",
       "      <td>65.907568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>19.954868</td>\n",
       "      <td>75.271849</td>\n",
       "      <td>7.013067</td>\n",
       "      <td>27.257663</td>\n",
       "      <td>18.653639</td>\n",
       "      <td>65.679849</td>\n",
       "      <td>1.604903</td>\n",
       "      <td>10.968427</td>\n",
       "      <td>10.188760</td>\n",
       "      <td>0</td>\n",
       "      <td>79.710447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>19.769678</td>\n",
       "      <td>73.029936</td>\n",
       "      <td>31.734303</td>\n",
       "      <td>22.191026</td>\n",
       "      <td>11.732328</td>\n",
       "      <td>60.767464</td>\n",
       "      <td>1.442845</td>\n",
       "      <td>26.598245</td>\n",
       "      <td>5.338447</td>\n",
       "      <td>0</td>\n",
       "      <td>67.748704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date        lat        lon    rain_mm     temp_c   wind_kph  \\\n",
       "0 2020-01-01  18.708241  74.350763  27.172204  26.853746  17.415451   \n",
       "1 2020-01-01  19.033959  74.442670  69.529848  29.004376  13.874468   \n",
       "2 2020-01-01  18.550919  73.632379  34.844697  27.977871  17.720272   \n",
       "3 2020-01-01  19.954868  75.271849   7.013067  27.257663  18.653639   \n",
       "4 2020-01-02  19.769678  73.029936  31.734303  22.191026  11.732328   \n",
       "\n",
       "    humidity  river_level_m  soil_moisture    evap_mm  flood_event  \\\n",
       "0  66.403607       1.496472      30.674063   6.343199            0   \n",
       "1  76.906338       3.319376      50.550377   8.022703            1   \n",
       "2  62.174314       1.714159      32.428313   8.671048            0   \n",
       "3  65.679849       1.604903      10.968427  10.188760            0   \n",
       "4  60.767464       1.442845      26.598245   5.338447            0   \n",
       "\n",
       "   drought_index  \n",
       "0      67.650480  \n",
       "1      49.862756  \n",
       "2      65.907568  \n",
       "3      79.710447  \n",
       "4      67.748704  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Synthetic Data Generator (Flood classification + Drought regression) ----\n",
    "# Geography box (e.g., a region in Maharashtra)\n",
    "lat_min, lat_max = 18.5, 20.0\n",
    "lon_min, lon_max = 73.0, 76.0\n",
    "\n",
    "n_points = 2500  # locations x dates\n",
    "start_date = datetime(2020, 1, 1)\n",
    "dates = [start_date + timedelta(days=i) for i in range(365)]\n",
    "\n",
    "# Sample lat/lon and assign each a random date (simulate daily observations)\n",
    "lats = np.random.uniform(lat_min, lat_max, n_points)\n",
    "lons = np.random.uniform(lon_min, lon_max, n_points)\n",
    "date_idx = np.random.randint(0, len(dates), n_points)\n",
    "date_series = np.array([dates[i] for i in date_idx])\n",
    "\n",
    "# Weather-like features\n",
    "rain_mm = np.clip(np.random.gamma(2.0, 15.0, n_points) - 5 + 25*np.random.binomial(1, 0.2, n_points), 0, None)\n",
    "temp_c = np.random.normal(28, 4, n_points) - 0.05*(rain_mm > 40)  # rainy days slightly cooler\n",
    "wind_kph = np.random.normal(15, 6, n_points) + 0.2*(rain_mm > 60) * np.random.normal(8, 3, n_points)\n",
    "humidity = np.clip(np.random.normal(60, 15, n_points) + 0.2*rain_mm, 10, 100)\n",
    "river_level_m = np.clip(1.2 + 0.02*rain_mm + np.random.normal(0, 0.3, n_points), 0, None)\n",
    "soil_moisture = np.clip(20 + 0.6*rain_mm - 0.3*temp_c + np.random.normal(0, 5, n_points), 5, 100)\n",
    "evap_mm = np.clip(5 + 0.2*temp_c - 0.05*humidity + np.random.normal(0, 1.5, n_points), 0, None)\n",
    "\n",
    "# Flood label (classification): high rainfall + river level + soil saturation\n",
    "flood_risk_score = 0.03*rain_mm + 1.5*river_level_m + 0.02*soil_moisture + 0.01*wind_kph - 0.02*temp_c\n",
    "flood_threshold = np.percentile(flood_risk_score, 80)  # top 20% risk = flood event\n",
    "flood_event = (flood_risk_score >= flood_threshold).astype(int)\n",
    "\n",
    "# Drought index (regression target): lower rainfall, high evap, low soil moisture -> higher drought index\n",
    "drought_index = np.clip(80 - 0.35*rain_mm + 0.6*evap_mm - 0.3*soil_moisture + np.random.normal(0, 3, n_points), 0, 100)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': date_series,\n",
    "    'lat': lats,\n",
    "    'lon': lons,\n",
    "    'rain_mm': rain_mm,\n",
    "    'temp_c': temp_c,\n",
    "    'wind_kph': wind_kph,\n",
    "    'humidity': humidity,\n",
    "    'river_level_m': river_level_m,\n",
    "    'soil_moisture': soil_moisture,\n",
    "    'evap_mm': evap_mm,\n",
    "    'flood_event': flood_event,\n",
    "    'drought_index': drought_index\n",
    "})\n",
    "\n",
    "df.sort_values('date', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Synthetic dataset shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035760c",
   "metadata": {},
   "source": [
    "### Optional: Save the synthetic dataset to CSV (for reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f3389a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\Users\\vedan\\Downloads\\synthetic_disaster_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"C:\\Users\\vedan\\Downloads\\synthetic_disaster_dataset.csv\")\n",
    "df.to_csv(data_path, index=False)\n",
    "print('Saved to:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4e69e",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering\n",
    "Create rolling features and lags per approximate spatial buckets (lat/lon rounding) to emulate time-series locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac79229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After FE: (0, 38)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>rain_mm</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>wind_kph</th>\n",
       "      <th>humidity</th>\n",
       "      <th>river_level_m</th>\n",
       "      <th>soil_moisture</th>\n",
       "      <th>evap_mm</th>\n",
       "      <th>flood_event</th>\n",
       "      <th>drought_index</th>\n",
       "      <th>lat_bucket</th>\n",
       "      <th>lon_bucket</th>\n",
       "      <th>rain_mm_roll3</th>\n",
       "      <th>rain_mm_roll7</th>\n",
       "      <th>rain_mm_roll14</th>\n",
       "      <th>temp_c_roll3</th>\n",
       "      <th>temp_c_roll7</th>\n",
       "      <th>temp_c_roll14</th>\n",
       "      <th>wind_kph_roll3</th>\n",
       "      <th>wind_kph_roll7</th>\n",
       "      <th>wind_kph_roll14</th>\n",
       "      <th>humidity_roll3</th>\n",
       "      <th>humidity_roll7</th>\n",
       "      <th>humidity_roll14</th>\n",
       "      <th>soil_moisture_roll3</th>\n",
       "      <th>soil_moisture_roll7</th>\n",
       "      <th>soil_moisture_roll14</th>\n",
       "      <th>evap_mm_roll3</th>\n",
       "      <th>evap_mm_roll7</th>\n",
       "      <th>evap_mm_roll14</th>\n",
       "      <th>river_level_m_roll3</th>\n",
       "      <th>river_level_m_roll7</th>\n",
       "      <th>river_level_m_roll14</th>\n",
       "      <th>rain_mm_lag1</th>\n",
       "      <th>river_level_m_lag1</th>\n",
       "      <th>soil_moisture_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, lat, lon, rain_mm, temp_c, wind_kph, humidity, river_level_m, soil_moisture, evap_mm, flood_event, drought_index, lat_bucket, lon_bucket, rain_mm_roll3, rain_mm_roll7, rain_mm_roll14, temp_c_roll3, temp_c_roll7, temp_c_roll14, wind_kph_roll3, wind_kph_roll7, wind_kph_roll14, humidity_roll3, humidity_roll7, humidity_roll14, soil_moisture_roll3, soil_moisture_roll7, soil_moisture_roll14, evap_mm_roll3, evap_mm_roll7, evap_mm_roll14, river_level_m_roll3, river_level_m_roll7, river_level_m_roll14, rain_mm_lag1, river_level_m_lag1, soil_moisture_lag1]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spatial buckets (approximate grid) to compute rolling stats by area\n",
    "df['lat_bucket'] = df['lat'].round(2)\n",
    "df['lon_bucket'] = df['lon'].round(2)\n",
    "\n",
    "df = df.sort_values(['lat_bucket','lon_bucket','date']).copy()\n",
    "\n",
    "def add_group_rolls(data, group_cols, feat, windows=(3,7,14)):\n",
    "    for w in windows:\n",
    "        col = f'{feat}_roll{w}'\n",
    "        data[col] = data.groupby(group_cols)[feat].transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
    "    return data\n",
    "\n",
    "for feat in ['rain_mm','temp_c','wind_kph','humidity','soil_moisture','evap_mm','river_level_m']:\n",
    "    df = add_group_rolls(df, ['lat_bucket','lon_bucket'], feat)\n",
    "\n",
    "# Example lags\n",
    "for feat in ['rain_mm','river_level_m','soil_moisture']:\n",
    "    df[f'{feat}_lag1'] = df.groupby(['lat_bucket','lon_bucket'])[feat].shift(1)\n",
    "\n",
    "df.dropna(inplace=True)  # remove rows where lags may be NaN after shifting\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print('After FE:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55fa06",
   "metadata": {},
   "source": [
    "## 4) Train/Test Split\n",
    "We split by time to avoid leakage for flood classification and drought regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c399c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (0, 38) Test size: (0, 38)\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (last 20% dates for test)\n",
    "split_time = df['date'].quantile(0.8)\n",
    "train_df = df[df['date'] <= split_time].copy()\n",
    "test_df  = df[df['date'] >  split_time].copy()\n",
    "\n",
    "print('Train size:', train_df.shape, 'Test size:', test_df.shape)\n",
    "\n",
    "# Common features\n",
    "base_feats = [c for c in df.columns if c not in ['date','flood_event','drought_index','lat_bucket','lon_bucket']]\n",
    "target_flood = 'flood_event'\n",
    "target_drought = 'drought_index'\n",
    "\n",
    "X_train_flood, y_train_flood = train_df[base_feats], train_df[target_flood]\n",
    "X_test_flood,  y_test_flood  = test_df[base_feats],  test_df[target_flood]\n",
    "\n",
    "X_train_drought, y_train_drought = train_df[base_feats], train_df[target_drought]\n",
    "X_test_drought,  y_test_drought  = test_df[base_feats],  test_df[target_drought]\n",
    "\n",
    "num_features = base_feats  # all are numeric here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6660a7e",
   "metadata": {},
   "source": [
    "## 5) Modeling — Flood Classification (High Accuracy with CV + Randomized Search)\n",
    "We use **GradientBoostingClassifier** (robust on tabular data) with strong cross-validation and randomized search for hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a634b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of folds=6 greater than the number of samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m\n\u001b[0;32m     20\u001b[0m tscv \u001b[38;5;241m=\u001b[39m TimeSeriesSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     21\u001b[0m search_flood \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     22\u001b[0m     flood_clf, param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     23\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mtscv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m search_flood\u001b[38;5;241m.\u001b[39mfit(X_train_flood, y_train_flood)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Flood params:\u001b[39m\u001b[38;5;124m'\u001b[39m, search_flood\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1992\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1993\u001b[0m         ParameterSampler(\n\u001b[0;32m   1994\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1995\u001b[0m         )\n\u001b[0;32m   1996\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1009\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[0;32m    997\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    998\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    999\u001b[0m         clone(base_estimator),\n\u001b[0;32m   1000\u001b[0m         X,\n\u001b[0;32m   1001\u001b[0m         y,\n\u001b[0;32m   1002\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m   1003\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m   1004\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   1005\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m   1006\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1289\u001b[0m, in \u001b[0;36mTimeSeriesSplit._split\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# Make sure we have enough samples for the given split parameters\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_folds \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m-> 1289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of folds=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1292\u001b[0m     )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m-\u001b[39m gap \u001b[38;5;241m-\u001b[39m (test_size \u001b[38;5;241m*\u001b[39m n_splits) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many splits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_splits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for number of samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with test_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and gap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of folds=6 greater than the number of samples=0."
     ]
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))  # tree models don't require scaling, but harmless\n",
    "])\n",
    "\n",
    "flood_clf = Pipeline(steps=[\n",
    "    ('prep', ColumnTransformer([('num', numeric_transformer, num_features)], remainder='drop')),\n",
    "    ('model', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [150, 250, 350, 450],\n",
    "    'model__learning_rate': np.linspace(0.02, 0.2, 10),\n",
    "    'model__max_depth': [2, 3, 4],\n",
    "    'model__min_samples_split': [2, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'model__subsample': [0.7, 0.85, 1.0]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_flood = RandomizedSearchCV(\n",
    "    flood_clf, param_distributions=param_distributions, n_iter=40,\n",
    "    scoring='roc_auc', cv=tscv, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "\n",
    "search_flood.fit(X_train_flood, y_train_flood)\n",
    "print('Best Flood params:', search_flood.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e234da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "best_flood = search_flood.best_estimator_\n",
    "proba = best_flood.predict_proba(X_test_flood)[:,1]\n",
    "preds = (proba >= 0.5).astype(int)\n",
    "\n",
    "print('ROC-AUC:', roc_auc_score(y_test_flood, proba))\n",
    "print('Accuracy:', accuracy_score(y_test_flood, preds))\n",
    "print('F1:', f1_score(y_test_flood, preds))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test_flood, preds))\n",
    "\n",
    "# Permutation importance (on a sampled subset for speed)\n",
    "idx = np.random.choice(len(X_test_flood), size=min(300, len(X_test_flood)), replace=False)\n",
    "r = permutation_importance(best_flood, X_test_flood.iloc[idx], y_test_flood.iloc[idx], n_repeats=5, random_state=42, scoring='roc_auc')\n",
    "imp_df = pd.DataFrame({'feature': num_features, 'importance': r.importances_mean}).sort_values('importance', ascending=False).head(15)\n",
    "imp_df.reset_index(drop=True, inplace=True)\n",
    "imp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c066e2",
   "metadata": {},
   "source": [
    "## 6) Modeling — Drought Index Regression (High Accuracy with CV + Randomized Search)\n",
    "We use **GradientBoostingRegressor** with strong validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_reg = Pipeline(steps=[\n",
    "    ('prep', ColumnTransformer([('num', numeric_transformer, num_features)], remainder='drop')),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_dist_reg = {\n",
    "    'model__n_estimators': [200, 300, 400, 600],\n",
    "    'model__learning_rate': np.linspace(0.02, 0.2, 10),\n",
    "    'model__max_depth': [2, 3, 4],\n",
    "    'model__min_samples_split': [2, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'model__subsample': [0.7, 0.85, 1.0]\n",
    "}\n",
    "\n",
    "search_drought = RandomizedSearchCV(\n",
    "    drought_reg, param_distributions=param_dist_reg, n_iter=40,\n",
    "    scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "search_drought.fit(X_train_drought, y_train_drought)\n",
    "print('Best Drought params:', search_drought.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "best_drought = search_drought.best_estimator_\n",
    "drought_pred = best_drought.predict(X_test_drought)\n",
    "mae = mean_absolute_error(y_test_drought, drought_pred)\n",
    "r2 = r2_score(y_test_drought, drought_pred)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "\n",
    "# Importance (permutation)\n",
    "idx2 = np.random.choice(len(X_test_drought), size=min(300, len(X_test_drought)), replace=False)\n",
    "r2imp = permutation_importance(best_drought, X_test_drought.iloc[idx2], y_test_drought.iloc[idx2], n_repeats=5, random_state=42, scoring='neg_mean_absolute_error')\n",
    "imp_reg_df = pd.DataFrame({'feature': num_features, 'importance': r2imp.importances_mean}).sort_values('importance', ascending=False).head(15)\n",
    "imp_reg_df.reset_index(drop=True, inplace=True)\n",
    "imp_reg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02253ff",
   "metadata": {},
   "source": [
    "## 7) Explainability — Top Features\n",
    "Bar charts of permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood importance plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(imp_df['feature'], imp_df['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Flood Model — Top Features (Permutation Importance)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Drought importance plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(imp_reg_df['feature'], imp_reg_df['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Drought Model — Top Features (Permutation Importance)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05cde1",
   "metadata": {},
   "source": [
    "## 8) Risk Scoring\n",
    "Combine flood probability and drought index into a unified **risk score** (0–100) and classify severity bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize drought index to 0-1, combine with flood probability\n",
    "flood_prob_test = best_flood.predict_proba(X_test_flood)[:,1]\n",
    "\n",
    "# Align indices (they come from the same test_df order)\n",
    "risk = pd.DataFrame({\n",
    "    'date': test_df['date'].values,\n",
    "    'lat': test_df['lat'].values,\n",
    "    'lon': test_df['lon'].values,\n",
    "    'flood_prob': flood_prob_test,\n",
    "    'drought_index': y_test_drought.values,  # ground truth\n",
    "    'drought_pred': drought_pred\n",
    "})\n",
    "# Composite risk: higher flood prob or higher drought_pred -> higher risk\n",
    "risk['comp_risk_0_100'] = 50*risk['flood_prob'] + 0.5*np.clip(risk['drought_pred'], 0, 100)\n",
    "# Severity bands\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "labels = ['Low','Moderate','High','Severe']\n",
    "risk['severity'] = pd.cut(risk['comp_risk_0_100'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "risk.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651040da",
   "metadata": {},
   "source": [
    "## 9) GIS — Risk Map (Folium)\n",
    "Interactive map with markers sized by risk and colored by severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map for severity\n",
    "def sev_color(s):\n",
    "    return {'Low':'green','Moderate':'orange','High':'red','Severe':'darkred'}.get(str(s),'blue')\n",
    "\n",
    "center_lat, center_lon = float(risk['lat'].mean()), float(risk['lon'].mean())\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "sample = risk.sample(min(500, len(risk)), random_state=42)  # limit markers for performance\n",
    "\n",
    "for _, row in sample.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=3 + (row['comp_risk_0_100']/20),\n",
    "        color=sev_color(row['severity']),\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Date: {row['date'].date()}\\nFlood prob: {row['flood_prob']:.2f}\\nDrought pred: {row['drought_pred']:.1f}\\nRisk: {row['comp_risk_0_100']:.1f} ({row['severity']})\"\n",
    "    ).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0374e99",
   "metadata": {},
   "source": [
    "## 10) Early Warning — Rule Engine\n",
    "Generates human-friendly alerts and suggested actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbe9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alerts(r):\n",
    "    alerts = []\n",
    "    for _, row in r.iterrows():\n",
    "        msg = None\n",
    "        if row['flood_prob'] >= 0.7:\n",
    "            msg = f\"⚠️ Flood risk HIGH ({row['flood_prob']:.2f}). Move to higher ground, prepare evacuation.\"\n",
    "        elif row['comp_risk_0_100'] >= 75:\n",
    "            msg = f\"⚠️ Severe composite risk ({row['comp_risk_0_100']:.1f}). Stay alert and follow local advisories.\"\n",
    "        elif row['comp_risk_0_100'] >= 50:\n",
    "            msg = f\"🔶 High composite risk ({row['comp_risk_0_100']:.1f}). Review safety plans.\"\n",
    "        if msg:\n",
    "            alerts.append({\n",
    "                'date': row['date'],\n",
    "                'lat': row['lat'],\n",
    "                'lon': row['lon'],\n",
    "                'message': msg\n",
    "            })\n",
    "    return pd.DataFrame(alerts)\n",
    "\n",
    "alerts_df = make_alerts(risk)\n",
    "print('Total alerts:', len(alerts_df))\n",
    "alerts_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a63257",
   "metadata": {},
   "source": [
    "## 11) Save Models & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2627e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_dir = Path('/mnt/data/models')\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_flood, art_dir / 'flood_model_gb.pkl')\n",
    "joblib.dump(best_drought, art_dir / 'drought_model_gb.pkl')\n",
    "risk.to_csv(art_dir / 'latest_risk_scores.csv', index=False)\n",
    "alerts_df.to_csv(art_dir / 'latest_alerts.csv', index=False)\n",
    "print('Saved models and CSVs to:', art_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ea46b",
   "metadata": {},
   "source": [
    "## 12) How to Use With Real Data\n",
    "1. Replace the **synthetic data generator** with your CSV(s).\n",
    "2. Ensure the following columns (or map your columns accordingly):\n",
    "   - `date` (YYYY-MM-DD), `lat`, `lon`\n",
    "   - `rain_mm`, `temp_c`, `wind_kph`, `humidity`, `river_level_m`, `soil_moisture`, `evap_mm`\n",
    "   - `flood_event` (0/1), `drought_index` (0–100)\n",
    "3. Re-run from **Feature Engineering** onward.\n",
    "\n",
    "### Tips for Higher Accuracy on Real Data\n",
    "- Increase `n_points` / dataset size.\n",
    "- Try `HistGradientBoostingClassifier/Regressor` or `CatBoost` (handles categorical/monotonic constraints well).\n",
    "- Add domain features: SPI (Standardized Precipitation Index), soil saturation days, river basin IDs.\n",
    "- Use **spatial cross-validation** (e.g., GroupKFold by basin) to avoid overfitting by location.\n",
    "- Calibrate probabilities with `CalibratedClassifierCV` for better alert thresholds.\n",
    "- Tune thresholds using precision-recall tradeoffs for imbalanced events.\n",
    "- Add seasonal features: month, monsoon flag, ENSO indices if available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
